{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking SCINet GeoCDL Options\n",
    "\n",
    "This notebook is designed to benchmark the throughput from various data sources to the USDA ARS HPC systems. Much of the code and inspiration for these metrics comes from a similiar effort via the Pangeo ([see this link](http://gallery.pangeo.io/repos/earthcube2020/ec20_abernathey_etal/cloud_storage.html) or this [preprint](https://www.authorea.com/doi/full/10.22541/au.160443768.88917719/v1))\n",
    "\n",
    "This benchmark uses dask arrays and a dask distributed computing system to scale the number of workers fetching data from remote data repositories. To run a benchmark:\n",
    "1. Copy the `template.ipynb` file.\n",
    "2. Rename the file like:<br>\n",
    "  * DataSource__FileType.ipynb (Note the double _ characters)\n",
    "  * e.g. `template.ipynb` --> `aws__netcdf.ipynb`\n",
    "  * e.g. `template.ipynb` --> `DukeFTP__Gdal_tif.ipynb`\n",
    "3. Fill in the IO code in the \"blank\" section.\n",
    "4. Run the entire notebook.\n",
    "5. Confirm a file was written into the result folder.\n",
    "\n",
    "**NOTE: You need to update the `container` variable in the 3rd cell below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from utils import DevNullStore,DiagnosticTimer,total_nthreads,total_ncores,total_workers,get_chunksize\n",
    "import time, datetime, os, dask\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "import dask_jobqueue as jq\n",
    "from tqdm.notebook import tqdm\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This environment set to optimized reading COG files - see https://github.com/pangeo-data/cog-best-practices\n",
    "\n",
    "env = dict(GDAL_DISABLE_READDIR_ON_OPEN='EMPTY_DIR', \n",
    "           AWS_NO_SIGN_REQUEST='YES',\n",
    "           GDAL_MAX_RAW_BLOCK_CACHE_SIZE='200000000',\n",
    "           GDAL_SWATH_SIZE='200000000',\n",
    "           VSI_CURL_CACHE_SIZE='200000000')\n",
    "\n",
    "os.environ.update(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition='brief-low'\n",
    "num_processes = 3\n",
    "num_threads_per_processes = 2\n",
    "mem = 5.2*num_processes*num_threads_per_processes#*1.25,\n",
    "n_cores_per_job = num_processes*num_threads_per_processes\n",
    "container = 'docker://rowangaffney/data_science_im_rs:latest'\n",
    "env = 'py_geo'\n",
    "cluster = jq.SLURMCluster(queue=partition,\n",
    "                          processes=num_processes,\n",
    "                          memory=str(mem)+'GB',\n",
    "                          cores=n_cores_per_job,\n",
    "                          interface='ib0',\n",
    "                          local_directory='$TMPDIR',\n",
    "                          death_timeout=30,\n",
    "                          python=\"singularity exec {} /opt/conda/envs/{}/bin/python\".format(container,env),\n",
    "                          walltime='01:30:00',\n",
    "                          scheduler_options={'dashboard_address': ':8777'},\n",
    "                          job_extra=[\"--output=/dev/null\",\"--error=/dev/null\"])\n",
    "client=Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Specific IO Code\n",
    "\n",
    "Load data into a dask array named `data`. Ideally subset the data is it has ~100MB chunks and totals ~25 GBs. An example approach is:\n",
    "```python\n",
    "ds = xr.open_rasterio('http://hydrology.cee.duke.edu/POLARIS/PROPERTIES/v1.0/vrt/clay_mode_0_5.vrt',\n",
    "                      chunks='auto')\n",
    "data = ds.isel(x=slice(0,5400*5),y=slice(0,5400*5)).data\n",
    "```\n",
    "\n",
    "Define the following metadata:\n",
    "\n",
    "**cloud_source** = Where is the data being housed <br>\n",
    "**format** = Format of the data. For gdal drivers use gdal_drivername (aka gdal_tif). For other sources, use the file formatt suffix (aka zarr).<br>\n",
    "**system** = The system (Ceres, Atlas, AWS, GCP, AZURE, etc...)\n",
    "\n",
    "Example:\n",
    "```python\n",
    "cloud_source = 'aws_nasa_daac'\n",
    "d_format = 'gdal_cog'\n",
    "system = 'Ceres'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ADD CODE BELOW ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = _\n",
    "d_format = _\n",
    "cloud_source = _\n",
    "system = _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Diagnostics on Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_timer = DiagnosticTimer()\n",
    "devnull = DevNullStore()\n",
    "\n",
    "chunksize = get_chunksize(data)\n",
    "totalsize = data.nbytes*1e-9\n",
    "\n",
    "diag_kwargs = dict(nbytes=data.nbytes,\n",
    "                   chunksize=chunksize,\n",
    "                   cloud_source=cloud_source,\n",
    "                   system=system,\n",
    "                   format=d_format)\n",
    "\n",
    "n_worker_lst = [3,6,12,18,30,42,66,90]\n",
    "runtime = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "for nworkers in tqdm(n_worker_lst):\n",
    "    client.restart()\n",
    "    cluster.scale(nworkers)\n",
    "    time.sleep(10)\n",
    "    client.wait_for_workers(nworkers)\n",
    "    with diag_timer.time(nthreads=total_nthreads(client),\n",
    "                         ncores=total_ncores(client),\n",
    "                         nworkers=total_workers(client),\n",
    "                         **diag_kwargs):\n",
    "\n",
    "        future = da.store(data, devnull, lock=False, compute=False)\n",
    "        dask.compute(future, retries=5)\n",
    "df = diag_timer.dataframe()\n",
    "cluster.scale(0)\n",
    "df['throughput_MBps'] = df.nbytes/1e6/df.runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize and save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save results\n",
    "df.to_csv('../results/Python__'+system+'__'+cloud_source+'__'+d_format+'__'+datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")+'.csv',index=False)\n",
    "\n",
    "#Plot  nworkers v. Throughput\n",
    "p = df.plot(x='nworkers',\n",
    "            y='throughput_MBps',\n",
    "            title='System: '+system+'    |    Data Source: '+cloud_source+'    |    File Format: '+d_format,\n",
    "            grid=True,\n",
    "            style='.-',\n",
    "            figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py_geo]",
   "language": "python",
   "name": "conda-env-py_geo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
